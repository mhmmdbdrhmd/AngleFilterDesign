
# Angle Sensor Filtering & Extremum Tracking

## Overview

This project focuses on transforming raw sensor readings—generated by left/right light‐sensor arrays—into a reliable hitch‐angle estimate by leveraging vehicle speed. The primary objective is to reconstruct a “true” hitch‐angle profile (from a high‐quality reference sensor) in real time, capturing sharp peaks and valleys accurately.

Rather than hard‐coding specific filenames, this workflow is designed to operate on **any new recording** (CSV file) you provide. As long as each CSV follows the same column conventions (left/right sensor sums, a reference angle column, a speed column, and a timestamp), the same preprocessing, filtering, alignment, and evaluation steps will apply.

---

## Directory Structure

```
.
├── README.md                  ← This file (generic instructions)
├── filter_analysis.py         ← Python script (load any CSV, process, plot, metrics)
└── recordings/                ← Place one or more CSV recordings here
    ├── recording_YYYYMMDD.csv ← Example recording file
    ├── another_recording.csv
    └── …
```

- **filter_analysis.py**: Contains all code for loading, preprocessing, filtering, alignment, and performance evaluation. It processes **one recording file at a time**, or can be adapted to loop over multiple files in the `recordings/` folder.
- **recordings/**: Directory where you place each new CSV log. Each file should include:
  - Two sets of light‐sensor sums (left vs. right),
  - A “hitch‐angle reference” column (the clean target),
  - A “speed” column (vehicle speed, in the same time base),
  - A “timestamp” column (e.g. “ESP Time” in milliseconds).

---

## 1. What We’re Trying to Accomplish

1. **Compute a raw hitch‐angle proxy** from left/right sensor sums.
2. **Clean the target reference** by interpolating any zero‐dropouts.
3. **Preprocess speed** by clipping negative values, interpolating zeros (only between first and last movement), and applying a small rolling‐average to remove jitter.
4. **Remove spikes** from the raw proxy with a causal Hampel filter (window=5).
5. **Apply a speed‐aware Kalman filter** (KF_inv) in real time, where process‐noise is scaled by 1/speed.
6. **Detect local extrema (peaks & valleys)** in the clean reference signal to quantify how well each filter preserves high/low points.
7. **Align** each filtered output to the reference by matching extremum indices, so that peak/valley errors can be measured at the correct time points.
8. **Compute metrics** (RMSE, MAE, plus specialized extrema‐MAE) to evaluate performance.
9. **Optionally test alternative filters** (e.g. Savitzky–Golay and hybrid KF_on_SG) on the same recording and compare results.

Because each recording file uses identical column names/patterns, you can simply update the path to the new CSV and re‐run the same script. No rewriting of “File1/File2/File3” is necessary.

---

## 2. Preprocessing Steps (Single Recording)

When you run the analysis script on any new recording:

1. **Load the CSV** into a DataFrame.
2. **Identify columns** via their column‐name patterns:
   - The “hitch‐angle reference” column matches a pattern like `Deichsel`.
   - The “speed” column matches a pattern like `Geschwindigkeit`.
   - Left‐sensor sums match `Durchschnitt_L_SE` and `Durchschnitt_L_Be_SE`.
   - Right‐sensor sums match `Durchschnitt_R_SE` and `Durchschnitt_R_Be_SE`.
3. **Compute the raw proxy**:
   - Define  
     ```
       L = (left‐sensor1 + left‐sensor2 + 16),
       R = (right‐sensor1 + right‐sensor2 + 16).
     ```
   - Compute  
     ```
       raw = 90 + (((L + R) / 2) * (L - R)) / (L * R) * 100.
     ```
4. **Clean the target**:
   - Replace zeros with NaN, then interpolate linearly (forward and backward).
5. **Preprocess speed**:
   - Clip negative values to zero.
   - Between the first and last nonzero sample, replace zeros with NaN and interpolate.
   - Apply a 5‐sample centered rolling mean, filling any NaNs at the ends with zero.
   - This yields a smooth, causal speed estimate for use in the Kalman filter.
6. **Suppress spikes** in the raw proxy using a **causal Hampel filter** (window=5).

---

## 3. Filtering Methods

Once preprocessing is complete, the script applies one or more of the following filters to `cleaned_raw`:

1. **KF_inv** (Speed‐Aware Kalman Filter)
2. **Savitzky–Golay (SG)**
3. **KF_on_SG** (KF_inv applied to SG output)

Customize by commenting/uncommenting the desired method calls in `filter_analysis.py`.

---

## 4. Extrema Detection & Alignment (Single Recording)

1. **Detect True Extrema**: local maxima/minima of `target_clean` with 10% prominence.
2. **Detect Filtered Extrema**: local maxima/minima of each filtered output with 10% of that series’ range.
3. **Align by Extrema**:
   - For each true extremum index, find nearest filtered extremum index, compute shift = (filtered_idx - true_idx).
   - Take median(shift) = `lag`. Circularly shift filtered output by `-lag`.
   - Now filtered peaks/valleys coincide with true peaks/valleys.

---

## 5. Scaling Optimization

After alignment, each filtered output can be scaled around a reference angle
close to 90°. Scaling is determined in two steps: first the reference angle is
selected by minimizing overall MAE (searching 80–100° in 1° steps with a range
of scale factors), then the scale factor is chosen to minimize
**Extrema_MAE** at the fixed reference. The chosen `ref_angle` and
`scale_k` values, along with the resulting `Extrema_MAE_scaled`, are recorded
for each filter.

---

## 6. Performance Metrics (Single Recording)

After alignment:
- **RMSE**: sqrt(mean squared error over all valid samples).
- **MAE**: mean absolute error over all valid samples.
- **MAPE_pk**: mean absolute error at true-peak indices.
- **MAVE_vl**: mean absolute error at true-valley indices.
- **Extrema_MAE**: mean absolute error over all true-peak+valley indices.
- **Extrema_MAE_scaled**: extrema MAE after the scaling optimization.
- **RMSE_scaled**: RMSE after scaling optimization.
- **MAE_scaled**: MAE after scaling optimization.

---

## 7. How to Use

1. **Place recording(s)** (CSV files) in `recordings/`.
2. **Edit `filter_analysis.py`**:
   - Set `file_path = "recordings/your_recording.csv"`.
   - Adjust `exclude_first_seconds` if needed (default = 20.0).
3. **Run**:
   ```
   python filter_analysis.py
   ```
   - The console prints performance metrics.
   - Two figures appear:
     1. **General + Alignment** (two‐subplot).
     2. **Detail View** (2×3).

4. **Add new recordings**:
   - Add more CSVs to `recordings/`.
   - Optionally loop over all files by modifying `filter_analysis.py`.
5. **Generated figures** are saved in `results/` as PNG files when you run the
   script. These images are reproducible and don't need to be committed to
   version control.

---

## 8. Next Steps & Customization

- Adjust prominence thresholds or filter parameters as needed.
- Explore additional causal smoothing methods.
- Automate batch processing for multiple recordings.

---
